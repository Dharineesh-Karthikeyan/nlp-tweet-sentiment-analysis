{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Neccesities \n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "#NLTK Packages\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I\\u2019m going to Chapel Hill on Sat. :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa and Johnny deal with him on Saturday.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just hate Nick Diaz. can\\u2019t wait for february.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome can\\u2019t deal with their missiles (keep talking like that and we may end up finding out)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establish Ties with the Mullahs http://t.co/TZZzrrKa via @PJMedia_com No Barack Obama - Vote Mitt Romney</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id sentiment  \\\n",
       "0  264183816548130816  positive   \n",
       "1  263405084770172928  negative   \n",
       "2  262163168678248449  negative   \n",
       "3  264249301910310912  negative   \n",
       "4  262682041215234048   neutral   \n",
       "\n",
       "                                                                                                                                        tweet_text  \n",
       "0                                                                           Gas by my house hit $3.39!!!! I\\u2019m going to Chapel Hill on Sat. :)  \n",
       "1                                                                Theo Walcott is still shit\\u002c watch Rafa and Johnny deal with him on Saturday.  \n",
       "2                                                       its not that I\\u2019m a GSP fan\\u002c i just hate Nick Diaz. can\\u2019t wait for february.  \n",
       "3          Iranian general says Israel\\u2019s Iron Dome can\\u2019t deal with their missiles (keep talking like that and we may end up finding out)  \n",
       "4  Tehran\\u002c Mon Amour: Obama Tried to Establish Ties with the Mullahs http://t.co/TZZzrrKa via @PJMedia_com No Barack Obama - Vote Mitt Romney  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/train.txt')\n",
    "pd.options.display.max_colwidth = 500\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(data['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoji's Dictionary\n",
    "emojis = {':-)':'happy',':)':'happy',':D':'happy',':o)':'happy',':]':'happy',':3':'happy',':c)':'happy', ':>':'happy', \n",
    "          '=]':'happy','8)':'happy',';)':'happy',';-)':'happy',':d':'happy',';d':'happy','xx':'happy','.xx':'happy',\n",
    "          '<33':'happy','<3':'happy','>.<':'happy','|;-)':'happy',\n",
    "          \n",
    "          '>:[':'sad',':-(':'sad',':(':'sad',':-c':'sad',':c':'sad',':-<':'sad',':<':'sad', ':-[':'sad',':[':'sad', \n",
    "          ':{':'sad',':’-(':'sad',':’(':'sad','>:':'sad','>:/':'sad',':-/':'sad',':-.':'sad',':- :':'sad','=/=':'sad',\n",
    "          ':L':'sad','=L':'sad',':S':'sad',\n",
    "          \n",
    "          ':-||':'angry',':@':'angry','>:(':'angry',\n",
    "    \n",
    "         'D:<':'shocked','D:':'shocked','D8':'shocked','D;':'shocked','D=':'shocked','DX':'shocked','v.v':'shocked',\n",
    "          'D-’':'shocked',';o':'shocked',';O':'shocked','XDD':'shocked','XD':'shocked','>:O':'shocked',':-O':'shocked',\n",
    "          ':O':'shocked',':-o':'shocked',':o':'shocked','8O':'shocked','O_O':'shocked','o-o':'shocked','O_o':'shocked',\n",
    "          'o_O':'shocked','o_o':'shocked','O-O':'shocked',\n",
    "          \n",
    "          ':|':'neutral',':-|':'netural',\n",
    "          \n",
    "         }\n",
    "\n",
    "\n",
    "# Contradictions and Abbreviations Dictionary\n",
    "\n",
    "replace = {\"dont\":\"do not\",\"don't\":\"do not\",\n",
    "              \"aren't\":\"are not\",\"arent\":\"are not\",\"cant\":\"can not\",\"can't\":\"can not\",\n",
    "              \"couldn't\":\"could not\",'couldn':\"could not\",\"couldnt\":\"could not\",\n",
    "              'didn':\"did not\",\"didn't\":\"did not\",\"didnt\":\"did not\",\n",
    "              'doesn':\"does not\",\"doesn't\":\"does not\",\"doesnt\":\"does not\",\n",
    "              'hadn':\"had not\",\"hadn't\":\"had not\",\"hadnt\":\"had not\",\n",
    "              'hasn':\"has not\", \"hasn't\":\"has not\", 'haven':\"have not\",\n",
    "              \"haven't\":\"have not\",\"haven\":\"have not\",\"havent\":\"have not\",\n",
    "              'isn':\"is not\", \"isn't\":\"is not\",\"isnt\":\"is not\",\n",
    "              'mightn':\"might not\",\"mightn't\":\"might not\",\"mightnt\":\"might not\",\n",
    "              'mustn':\"must not\", \"mustn't\":\"must not\",\"mustnt\":\"must not\",\n",
    "              'needn':\"need not\",\"needn't\":\"need not\",\"neednt\":\"need not\",\n",
    "              'shan':\"shall not\", \"shan't\":\"shall not\",\"shant\":\"shall not\",\n",
    "              'shouldn':\"should not\", \"shouldn't\":\"should not\",\"shouldnt\":\"should not\",\n",
    "              'wasn':\"was not\",\"wasn't\":\"was not\",\"wasnt\":\"was not\",\n",
    "              'weren':\"were not\", \"weren't\":\"were not\",\"werent\":\"were not\",\n",
    "              'won':'will not', \"won't\":\"will not\",\"wont\":\"will not\",\n",
    "              'wouldn':\"would not\",\"wouldn't\":\"would not\",\"wouldnt\":\"would not\",\n",
    "              \"aint\":\"am not\",\"ain't\":\"am not\",'im':'i am',\"i'm\":'i am',\n",
    "              'yall':'you all',\"youve\":'you have',\"you've\":'you have',\"i'll\":'i will','ill':'i will','u':'you',\n",
    "              'amp':'is not my problem','smh':'shaking my head','lol':'laughing out loud',\n",
    "              'jan':'january','feb':'february','mar':'march','apr':'april','jun':'june','aug':'august',\n",
    "              'nov':'november','sept':'september','oct':'october','dec':'december',\n",
    "              'mon':'monday','tue':'tuesday','tues':'tuesday','wed':'wednesday','thur':'thursday','fri':'friday',\n",
    "              '@':'at','tis':'this','ohlawd':'oh lord','yas':'yes','yass':'yes','hmm':'',\n",
    "              'tho':'though','thov':'though','btw':'by the way',\"cmon\":'come on',\"c'mon\":'come on','cuz':'cause',\n",
    "              'diff':'difference','sayin':'saying','drivin':'driving',\n",
    "              'lmfao':'laughing my fucking ass off','lmao':'laughing my ass off','rofl':'rolling on the floor laughing',\n",
    "              'lmaoo':'laughing my ass off'}\n",
    "\n",
    "\n",
    "# Stop Words \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "stop_words.append('tomorrow')\n",
    "stop_words.append('today')\n",
    "stop_words.extend(['monday','tuesday','wednesday','thursday','friday','saturday','sunday','january','february','march','april','may','june','july','august','september','october','november','december'])\n",
    "\n",
    "\n",
    "# Lemmatization with Parts of Speech Tagging\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Parts of Speech Tagging\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Function\n",
    "def cleaning(text):\n",
    "    clean=[]\n",
    "    i=0\n",
    "    for tweet in text:\n",
    "        i+=1\n",
    "        if i%5000==0:\n",
    "            print('Completed:',i)\n",
    "        if r'\\u2019' in tweet: \n",
    "            tweet = tweet.replace('\\\\u2019',\"'\")\n",
    "        if r'\\u002c' in tweet:\n",
    "            tweet= tweet.replace('\\\\u002c',',') \n",
    "        if r'\\\"\"' in tweet:\n",
    "            tweet= tweet.replace('\\\"\"','\"') \n",
    "        if '&amp;' in tweet:\n",
    "            tweet= tweet.replace('&amp;','&')\n",
    "\n",
    "        \n",
    "        tweet = re.sub(r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*|(http)|(http:/))\",'',tweet)\n",
    "        \n",
    "        tweet = re.sub('RT','',tweet)\n",
    "        \n",
    "        tweet = re.sub('@[A-Za-z0-9_]+','', tweet) \n",
    "        \n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        tweet = re.sub(r\"(.)\\1\\1+\", r\"\\1\\1\", tweet)\n",
    "        \n",
    "        tweet = re.sub(r\"(\\d+th)|(\\d+rd)|(\\d+st)|(\\d+nd)|(\\d+)\",'',tweet)\n",
    "        \n",
    "        emojis_list = emojis.keys()\n",
    "        for emoji in emojis_list:\n",
    "            if emoji in tweet:\n",
    "                tweet=tweet.replace(emoji,' '+ emojis[emoji])\n",
    "            \n",
    "        tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "              \n",
    "        tweet_text =''\n",
    "        for word in tweet.split():\n",
    "            if len(word)>6:\n",
    "                word = wordninja.split(word)\n",
    "                word = ' '.join(word)\n",
    "            tweet_text += (word+' ')\n",
    "            \n",
    "        tweet = lemmatize_sentence(tweet_text)\n",
    "        \n",
    "        tweet_text =''\n",
    "        for word in tweet.split():\n",
    "            if word in replace:\n",
    "                word = replace[word]\n",
    "            for wd in word.split():\n",
    "                if wd not in stop_words:\n",
    "                    tweet_text += (word+' ')\n",
    "        tweet = tweet_text\n",
    "        clean.append(tweet)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 5000\n",
      "Completed: 10000\n",
      "Completed: 15000\n",
      "Completed: 20000\n"
     ]
    }
   ],
   "source": [
    "clean_text = cleaning(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I\\u2019m going to Chapel Hill on Sat. :)</td>\n",
       "      <td>gas house hit go chapel hill sat happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa and Johnny deal with him on Saturday.</td>\n",
       "      <td>theo walcott still shit watch rafa johnny deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just hate Nick Diaz. can\\u2019t wait for february.</td>\n",
       "      <td>not gsp fan hate nick diaz can not wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome can\\u2019t deal with their missiles (keep talking like that and we may end up finding out)</td>\n",
       "      <td>iranian general say israel iron dome can not deal missile keep talk like end find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establish Ties with the Mullahs http://t.co/TZZzrrKa via @PJMedia_com No Barack Obama - Vote Mitt Romney</td>\n",
       "      <td>tehran amour obama try establish tie mullah via no barack obama vote mitt romney</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id sentiment  \\\n",
       "0  264183816548130816  positive   \n",
       "1  263405084770172928  negative   \n",
       "2  262163168678248449  negative   \n",
       "3  264249301910310912  negative   \n",
       "4  262682041215234048   neutral   \n",
       "\n",
       "                                                                                                                                        tweet_text  \\\n",
       "0                                                                           Gas by my house hit $3.39!!!! I\\u2019m going to Chapel Hill on Sat. :)   \n",
       "1                                                                Theo Walcott is still shit\\u002c watch Rafa and Johnny deal with him on Saturday.   \n",
       "2                                                       its not that I\\u2019m a GSP fan\\u002c i just hate Nick Diaz. can\\u2019t wait for february.   \n",
       "3          Iranian general says Israel\\u2019s Iron Dome can\\u2019t deal with their missiles (keep talking like that and we may end up finding out)   \n",
       "4  Tehran\\u002c Mon Amour: Obama Tried to Establish Ties with the Mullahs http://t.co/TZZzrrKa via @PJMedia_com No Barack Obama - Vote Mitt Romney   \n",
       "\n",
       "                                                                           clean_text  \n",
       "0                                             gas house hit go chapel hill sat happy   \n",
       "1                                     theo walcott still shit watch rafa johnny deal   \n",
       "2                                            not gsp fan hate nick diaz can not wait   \n",
       "3  iranian general say israel iron dome can not deal missile keep talk like end find   \n",
       "4   tehran amour obama try establish tie mullah via no barack obama vote mitt romney   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text']=clean_text\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Clean Dataset\n",
    "\n",
    "data.to_csv('data/train_clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264238274963451904</td>\n",
       "      <td>@jjuueellzz down in the Atlantic city, ventnor, margate, ocean city area. I'm just waiting for the coordinator to hopefully call me tomorrow</td>\n",
       "      <td>atlantic city ventnor margate ocean city area wait coordinator hopefully call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218775148495515649</td>\n",
       "      <td>Musical awareness: Great Big Beautiful Tomorrow has an ending, Now is the time does not</td>\n",
       "      <td>musical awareness great big beautiful end time not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258965201766998017</td>\n",
       "      <td>On Radio786 100.4fm 7:10 Fri Oct 19 Labour analyst Shawn Hattingh: Cosatu's role in the context of unrest in the mining http://t.co/46pjzzl6</td>\n",
       "      <td>radio fm labour analyst shawn hat h cosatu role context unrest mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262926411352903682</td>\n",
       "      <td>Kapan sih lo ngebuktiin,jan ngomong doang Susah Susah.usaha Aja blm udh nyerah,inget.if you never try you'll never know.cowok kok gentle bgt</td>\n",
       "      <td>kapan sih lo nge buk ti ngo mong doang susah susa hu saha aja blm udh nye rah get never try youll never know co wok kok gentle bgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171874368908050432</td>\n",
       "      <td>Excuse the connectivity of this live stream, from Baba Amr, so many activists using only one Sat Modem. LIVE http://t.co/U283IhZ5 #Homs</td>\n",
       "      <td>excuse connectivity live stream baba amr many activist use one sit modem live homs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  \\\n",
       "0  264238274963451904   \n",
       "1  218775148495515649   \n",
       "2  258965201766998017   \n",
       "3  262926411352903682   \n",
       "4  171874368908050432   \n",
       "\n",
       "                                                                                                                                     tweet_text  \\\n",
       "0  @jjuueellzz down in the Atlantic city, ventnor, margate, ocean city area. I'm just waiting for the coordinator to hopefully call me tomorrow   \n",
       "1                                                       Musical awareness: Great Big Beautiful Tomorrow has an ending, Now is the time does not   \n",
       "2  On Radio786 100.4fm 7:10 Fri Oct 19 Labour analyst Shawn Hattingh: Cosatu's role in the context of unrest in the mining http://t.co/46pjzzl6   \n",
       "3  Kapan sih lo ngebuktiin,jan ngomong doang Susah Susah.usaha Aja blm udh nyerah,inget.if you never try you'll never know.cowok kok gentle bgt   \n",
       "4       Excuse the connectivity of this live stream, from Baba Amr, so many activists using only one Sat Modem. LIVE http://t.co/U283IhZ5 #Homs   \n",
       "\n",
       "                                                                                                                            clean_text  \n",
       "0                                                       atlantic city ventnor margate ocean city area wait coordinator hopefully call   \n",
       "1                                                                                  musical awareness great big beautiful end time not   \n",
       "2                                                               radio fm labour analyst shawn hat h cosatu role context unrest mining   \n",
       "3  kapan sih lo nge buk ti ngo mong doang susah susa hu saha aja blm udh nye rah get never try youll never know co wok kok gentle bgt   \n",
       "4                                                  excuse connectivity live stream baba amr many activist use one sit modem live homs   "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data - Cleaning Process:\n",
    "\n",
    "test = pd.read_csv('data/test_samples.txt')\n",
    "test_tweets = list(test['tweet_text'])\n",
    "test_clean_text = cleaning(test_tweets)\n",
    "test['clean_text']= test_clean_text\n",
    "test.to_csv('data/test_clean_data.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
